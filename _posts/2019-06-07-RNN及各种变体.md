---
layout:     post   				    # 使用的布局（不需要改）
title:      RNN及各种变体				# 标题 
date:       2019-06-07 				# 时间
author:     0point618					# 作者
catalog: true 						# 是否归档
tags:								#标签
    - 理论
---



##RNN

### 提出的原因

之前的全连接以及卷积神经网络没有**记忆性**。

### 网络结构和计算过程

![RNN](http://ww3.sinaimg.cn/large/006tNc79ly1g41miaq85jj31400u0ab2.jpg)

输入：x

记忆：s

输出：o

变量之间的关系：

$S_{t}=f\left(U * X_{t}+W * S_{t-1}\right)$

$o_{t}=\operatorname{softmax}\left(V S_{t}\right)$

在pytorch中：

$h_{t}=\tanh \left(W_{i h} x_{t}+b_{i h}+W_{h h} h_{(t-1)}+b_{h h}\right)$

其中(这里batch first=True, 关于为什么有batch first，和cudnn有关，参见[读PyTorch源码学习RNN（1）](https://zhuanlan.zhihu.com/p/32103001)）

x: [batch size, seq len, input size]

Wih: [batch size, hidden size, input size]

Whh: [batch size, hidden size, hidden size]

bih: [batch size, hidden size]

bih: [batch size, hidden size]

output, hn = torch.nn.RNN(input) 

output: [batch size, seq len, num_directions*hidden size]

i.e. the output tensor containing the output features (h_t) from the last layer of the RNN, for each t

hn: [batch size, num_layers*num_directions, hidden size]

i.e. the hn tensor containing the hidden state for t = seq_len.

我们可以发现，在实际的代码中没有出现前面RNN理论的图中的V和每个时间点t的输出o。参见[读PyTorch源码学习RNN（1）](https://zhuanlan.zhihu.com/p/32103001)

![image-20190608092739618](http://ww1.sinaimg.cn/large/006tNc79ly1g41mjx4gwej313i08iwhv.jpg)

[RNN中为什么要采用tanh而不是ReLu作为激活函数？](https://www.zhihu.com/question/61265076)

## LSTM

[LSTM原理与实践，原来如此简单](https://zhuanlan.zhihu.com/p/51870057)

## GRU















